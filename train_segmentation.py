# -*- coding: utf-8 -*-
"""train_segmentation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ou1B2-LZ2-37g0hCIZYUb_VzwdJU_QaO

# Set up Path
"""

import kagglehub
# Download latest version
path = kagglehub.dataset_download("balraj98/deepglobe-land-cover-classification-dataset")

print("Path to dataset files:", path)

# Commented out IPython magic to ensure Python compatibility.
# Prepare GoogleColab
from google.colab import drive
drive.mount('/content/drive')

import os
GOOGLE_DRIVE_PATH_POST_MYDRIVE = "Gatech_OMSCS/2025Summer/CS7643/group_project"
GOOGLE_DRIVE_PATH = os.path.join('/content', 'drive', 'MyDrive', GOOGLE_DRIVE_PATH_POST_MYDRIVE)
print(os.listdir(GOOGLE_DRIVE_PATH))

import sys
if 'google.colab' in sys.modules:
  print(f'Running in google colab. Our path is `{GOOGLE_DRIVE_PATH}`')
else:
  GOOGLE_DRIVE_PATH = '.'
  print('Running locally.')

# %cd {GOOGLE_DRIVE_PATH}

"""#Download loss function"""

# 1. Clean up by removing any conflicting file named 'losses'
# The -f flag prevents an error if the file doesn't exist.
!rm -f losses

# 2. Create the 'losses' directory.
# The -p flag prevents an error if the directory already exists.
!mkdir -p losses

# 3. Create the __init__.py file inside the new directory
!touch losses/__init__.py

# --- Download all files directly into the 'losses' directory ---

# List of loss files to download
files = [
    "combo.py",
    "dice.py",
    "focal.py",
    "focal_tversky.py",
    "jaccard.py",
    "lovasz_softmax.py",
    "topk.py",
    "tversky.py",
    "utils.py"
]

# Base URL of the repository
base_url = "https://raw.githubusercontent.com/YilmazKadir/Segmentation_Losses/main/losses/"

# Download each file into the './losses/' directory
for file in files:
    # Use the -P flag to specify the output directory
    !wget -q -P ./losses/ {base_url}{file}

print("\n✅ All loss function files downloaded correctly into the 'losses' folder.")

"""## Import classes and funcs"""

try:
    import torch._dynamo
    torch._dynamo.disable()
    print("✅ TorchDynamo has been disabled.")
except (AttributeError, ImportError):
    print("⚠️ Could not disable TorchDynamo (may be an older PyTorch version).")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import torchvision.transforms as transforms
from PIL import Image
import os
from datetime import datetime
import json
import warnings
import pandas as pd
import seaborn as sns
import numpy as np
import torch.cuda.amp as amp

warnings.filterwarnings("ignore")

# --- Import modules from your project ---
from unet_model import UNet
from dataset import DeepGlobeDataset, PatchedDeepGlobeDataset
from utils import (
    TrainingPipeline,
    get_loss_function,
    create_transforms_from_config,
    visualize_predictions,
    create_comparison_plots,
    PixelMetrics,
    denormalize,
    colorize_mask
)

print("Modules loaded successfully!")

"""# Model Configuration"""

BASE_CONFIG = {
    'data_root': '/kaggle/input/deepglobe-land-cover-classification-dataset',
    'num_classes': 7,
    'image_size': 256,
    'batch_size': 15,
    'num_workers': 16,
    'num_epochs': 20,
    'loss_function': 'focal_tversky',
    'train_split': 'train',
    'train_val_split_ratio': 0.8,
    'visualize_samples': 160,
    'focus_hyperparameter': 'loss_function',

    'optimizer': {
        'type': 'adam',
        'lr': 0.001,
        'weight_decay': 1e-5
    },

    'scheduler': {
        'type': 'step',
        'params': {'step_size': 10, 'gamma': 0.5}
    },

    'augmentation': {
        'horizontal_flip': 0.5,
        'vertical_flip': 0.5,
        'color_jitter': {'brightness': 0.2, 'contrast': 0.2, 'saturation': 0.2}
    },

    'normalization': {
        'mean': [0.485, 0.456, 0.406],
        'std': [0.229, 0.224, 0.225]
    },

    'save_plots': True,
    'save_history': True,
}

RUNS_TO_PERFORM = [
    # Example 1: Default config (can remove this if not needed)
    # {},

    # Example 2: Different loss function
    {'loss_function': 'focal_tversky', 'optimizer': {'lr': 0.0005, 'weight_decay': 0.00005}, 'num_epochs': 25},

    # Example 3: Another loss function with different LR
    # {'loss_function': 'jaccard', 'optimizer': {'lr': 0.0001}, 'num_epochs': 30},

    # Example 4: Testing a different optimizer type
    # {'optimizer': {'type': 'sgd', 'lr': 0.01, 'momentum': 0.9}},

    # Example 5: Shorter training for quick test
    # {'num_epochs': 5, 'batch_size': 32},
]

"""#Training pipeline"""

def run_single_experiment(config, experiment_path):
    """
    Executes a single training and validation pipeline based on the provided config.
    """
    # Set seeds for reproducibility
    torch.backends.cudnn.deterministic = True
    torch.manual_seed(42)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}')

    # Clear CUDA cache at the start of each run for memory management
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("CUDA cache cleared at start of run.")

    # Create data transforms
    train_transform, val_transform, target_transform = create_transforms_from_config(config)

    # Initialize the full dataset
    full_dataset = PatchedDeepGlobeDataset(
        root_dir=config['data_root'],
        transform=train_transform,
        target_transform=target_transform
    )

    total_size = len(full_dataset)
    train_ratio = 0.8  # 80% for training
    val_ratio = 0.2    # 20% for validation

    # Calculate sizes for the 80/20 split
    train_size = int(train_ratio * total_size)
    val_size = total_size - train_size  # Ensure the sizes sum up to the total

    print(f"Total dataset size: {total_size}")
    print(f"Splitting into: Train ({train_size}), Validation ({val_size})")

    # Perform the random split
    generator = torch.Generator().manual_seed(42)
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size], generator=generator
    )

    # IMPORTANT: Assign appropriate transforms to each split.
    # The 'full_dataset' was initialized with train_transform, so we need to
    # explicitly set val_transform for the val_dataset.
    val_dataset.dataset.transform = val_transform

    # Create DataLoaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=config['num_workers'],
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        pin_memory=True
    )

    print(f'Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}')

    # Initialize model, criterion, optimizer, and scheduler
    model = UNet(in_channels=3, num_classes=config['num_classes']).to(device)
    criterion = get_loss_function(config['loss_function'])

    optimizer_class = getattr(optim, config['optimizer']['type'].capitalize(), None)
    if optimizer_class is None:
        raise ValueError(f"Optimizer type '{config['optimizer']['type']}' not recognized.")

    optimizer_params = {k: v for k, v in config['optimizer'].items() if k != 'type'}
    optimizer = optimizer_class(model.parameters(), **optimizer_params)

    scheduler = None
    if config['scheduler']['type'] == 'step':
        scheduler = optim.lr_scheduler.StepLR(optimizer, **config['scheduler']['params'])
    elif config['scheduler']['type'] is not None:
        print(f"Warning: Scheduler type '{config['scheduler']['type']}' not implemented. No scheduler will be used.")

    # Initialize the training pipeline
    pipeline = TrainingPipeline(
        model=model, train_loader=train_loader, val_loader=val_loader,
        criterion=criterion, optimizer=optimizer, scheduler=scheduler,
        device=device, num_classes=config['num_classes']
    )

    # Define file path for saving the best model
    model_save_path = os.path.join(experiment_path, 'best_model.pth')

    # Start training
    pipeline.train(num_epochs=config['num_epochs'], model_save_path=model_save_path)

    # --- EVALUATE ON THE FINAL VALIDATION SET ---
    print("\n--- Evaluating on the Final Validation Set ---")
    final_val_metrics = {}
    if os.path.exists(model_save_path):
        print(f"Loading best model from {model_save_path} for final validation evaluation.")
        model.load_state_dict(torch.load(model_save_path, map_location=device))
        model.eval()

        # We pass the val_loader here, as there's no separate test set.
        final_val_metrics = pipeline.evaluate(val_loader)

        # Print detailed validation metrics
        print("Final Validation Set Results:")
        print(f"  Loss: {final_val_metrics['loss']:.4f}")
        print(f"  Overall Accuracy: {final_val_metrics['overall_accuracy']:.4f}")
        print(f"  Mean IoU (mIoU): {final_val_metrics['mIoU']:.4f}")
        print(f"  Mean F1 (mF1): {final_val_metrics['mF1']:.4f}")

        # Class names for DeepGlobe (adjust if your class mapping differs)
        class_names = ["Unknown", "Urban", "Agriculture", "Rangeland", "Forest", "Water", "Barren"]
        print("\n  Per-Class Metrics on Validation Set:")
        for i, class_name in enumerate(class_names):
            if i < len(final_val_metrics['per_class_iou']):
                print(f"    Class {i} ({class_name}):")
                print(f"      IoU: {final_val_metrics['per_class_iou'][i]:.4f}")
                print(f"      F1: {final_val_metrics['per_class_f1'][i]:.4f}")
                print(f"      Recall (Pixel Accuracy): {final_val_metrics['per_class_recall'][i]:.4f}")
            else:
                print(f"    Class {i} ({class_name}): No data or invalid index.")
    else:
        print("Warning: Best model not found for validation evaluation. Skipping evaluation.")

    # --- Post-Training Actions (Saving & Visualization) ---
    if config['save_plots']:
        pipeline.plot_metrics(save_path=experiment_path)

    if config['save_history']:
        history_file_path = os.path.join(experiment_path, 'training_history.json')
        serializable_config = config.copy()

        # Prepare a serializable config for JSON
        if 'optimizer' in serializable_config:
            serializable_config['optimizer'] = {k: v for k, v in serializable_config['optimizer'].items()}
        if 'scheduler' in serializable_config and 'params' in serializable_config['scheduler']:
            serializable_config['scheduler']['params'] = {k: v for k, v in serializable_config['scheduler']['params'].items()}

        full_history_data = {
            'train_history': pipeline.history,
            'final_val_metrics': final_val_metrics,  # Changed to val metrics
            'config': serializable_config
        }
        with open(history_file_path, 'w') as f:
            json.dump(full_history_data, f, indent=2)
            print(f"Saved training history and final validation metrics to {history_file_path}")

    # Visualize predictions using the best model (on validation data now)
    if config.get('visualize_samples', 0) > 0 and os.path.exists(model_save_path):
        print("\nGenerating validation set prediction visualizations...")
        visualize_predictions(
            model=model,
            dataset=val_dataset,  # Now visualizing on the validation set
            device=device,
            config=config,
            save_path=os.path.join(experiment_path, 'val_visualizations')
        )
    else:
        print("Skipping validation set visualization.")

    # Collect and return final metrics for comparison table (for CSV)
    final_metrics = {
        'train_loss': pipeline.history['train_loss'][-1] if pipeline.history['train_loss'] else None,
        'val_loss': pipeline.history['val_loss'][-1] if pipeline.history['val_loss'] else None,
        'val_overall_accuracy': pipeline.history['val_overall_accuracy'][-1] if pipeline.history['val_overall_accuracy'] else None,
        'val_mIoU': pipeline.history['val_mIoU'][-1] if pipeline.history['val_mIoU'] else None,
        'val_mF1': pipeline.history['val_mF1'][-1] if pipeline.history['val_mF1'] else None,
        'best_val_mIoU': pipeline.best_val_mIoU,

        # Changed to report final validation metrics
        'final_val_loss': final_val_metrics.get('loss'),
        'final_val_overall_accuracy': final_val_metrics.get('overall_accuracy'),
        'final_val_mIoU': final_val_metrics.get('mIoU'),
        'final_val_mF1': final_val_metrics.get('mF1'),

        'final_val_per_class_iou': json.dumps(final_val_metrics.get('per_class_iou')) if final_val_metrics.get('per_class_iou') else None,
        'final_val_per_class_f1': json.dumps(final_val_metrics.get('per_class_f1')) if final_val_metrics.get('per_class_f1') else None,
        'final_val_per_class_recall': json.dumps(final_val_metrics.get('per_class_recall')) if final_val_metrics.get('per_class_recall') else None,

        'num_epochs_trained': len(pipeline.history['train_loss']) if pipeline.history['train_loss'] else 0,

        **{k: v for k, v in config.items() if k not in ['optimizer', 'scheduler', 'augmentation', 'normalization', 'train_val_split_ratio', 'train_split']},
        'optimizer_type': config['optimizer']['type'],
        'optimizer_lr': config['optimizer']['lr'],
        'optimizer_weight_decay': config['optimizer'].get('weight_decay', 0),
        'scheduler_type': config['scheduler']['type'],
        'scheduler_step_size': config['scheduler']['params'].get('step_size'),
        'scheduler_gamma': config['scheduler']['params'].get('gamma'),
    }
    return final_metrics

# Define a path for this single experiment's results
single_run_path = F"{GOOGLE_DRIVE_PATH}/results/final"
os.makedirs(single_run_path, exist_ok=True)

print("Running single experiment with BASE_CONFIG directly...")
results = run_single_experiment(BASE_CONFIG, single_run_path)
print("Single experiment finished. Results:", results)